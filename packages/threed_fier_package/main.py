"""
this script defines function in order to generate the datasets  that will be taken input by 3DFier in order to geenrate the LOD-2 level 
cityGML file.
"""
import yaml
import json
from subprocess import check_call
import argparse
import os

"""
function to generate the yaml configuration file as input to the 3dfier
"""

def write_yaml_file(config_json, config_file_name):    
    with open(config_file_name, 'w') as fil:
        yaml.dump(config_json, fil, sort_keys=True)
        
def create_config(input_polygons: any, input_elevation: any, lifting_options: any, options: any):
  config = {}

  # Input polygons
  config["input_polygons"] = []
  for polygon in input_polygons:
    config["input_polygons"].append({
      "datasets": polygon["datasets"],
      "uniqueid": polygon["uniqueid"],
      "lifting": polygon["lifting"],
      "height_field": polygon.get("height_field"),
      "handle_multiple_heights": polygon.get("handle_multiple_heights"),
    })

  # Input elevation
  config["input_elevation"] = []
  for elevation in input_elevation:
    config["input_elevation"].append({
      "datasets": elevation["datasets"],
      "omit_LAS_classes": elevation.get("omit_LAS_classes"),
      "thinning": elevation.get("thinning"),
    })

  # Lifting options
  config["lifting_options"] = lifting_options

  # Options
  config["options"] = options

  return config


def store_datasets(laz_file_url: str, datasets:str ):
  """
  downloads the pointcloud laz files along with the corresponding digital height map and metadata format
  laz_file_url: url for  the laz file to be regenerated
  datasets: url of the sql and other storage files for LoD generation and normalization.
  NOTE: right now we've hardcoded the associated datasets (from netherlands) but once the data pipeline is developed we can implement for all given functions
  """
  try:
    if not os.path.exists(__file__ + './ahn3/') or os.path.exists(__file__ + './bgt/'):
      os.mkdir(__file__ + './ahn3/')
      os.mkdir(__file__ + './bgt/')
    check_call(['curl -o '+ __file__ + './ahn3/ ' + laz_file_url])
    check_call(['curl -o '+ __file__ + './bgt/ ' + laz_file_url])
  except Exception as e:
    print(str(e))

def run_threeDFier(output_object:str, output_cityJson:str, outpput_cityGML:str, yml_file):
    """
    function to run the 3Dfier pipeline and generate the City and objective pipeline
    output_obj: dir of the yml file that is to be parsed by input
    output_cityJSON: dir of the CityJSON file format 
    
    """
    try:
      check_call(['3dfier ' + '' + yml_file + ' --OBJ ' + output_object + ' --CityJSON ' + output_cityJson + ' --CityGML ' + outpput_cityGML]) 
    except Exception as e:
      print('exception in run_3dfier' + str(e))

                                                                                                 
if __name__ == "__main__":
    parameters = argparse.ArgumentParser("run cityGML generation pipeline using 3Dfier 3D model creation tool ")
    parameters.add_argument('-f',"--function", choices=["create_config", "write_yaml_file", "run_threeDFier"], type=str)
    parameters.add_argument("-xml","--yaml_file", type=str, help="defines the path of the given yml configuration generated by the function")
    parameters.add_argument("--OBJ", type=str, help="output dir of the resulting obj file" )
    parameters.add_argument("--CityJSON", type=str, help="output dir of the resulting cityJSON file")
    parameters.add_argument("--CityGML", type=str, help="output dir of the resulting cityGML file")
    parameters.add_argument("-l,--LAZ", type=str, action="store_true", help="defines the laz files that are to be evaluated")
    parameters.add_argument('-d,--Dataset', type=str, help="defines the dataset files (storing the alignment and other geopositional arrangement) for LoD generation" )
    args = parameters.parse_args()
    
    if args.function == "run_threeDFier":
      run_threeDFier(args.OBJ, args.CityJSON, args.CityGML, args.yaml_file)
    if args.function == "store_datasets":
      store_datasets(args.LAZ, args.Dataset)